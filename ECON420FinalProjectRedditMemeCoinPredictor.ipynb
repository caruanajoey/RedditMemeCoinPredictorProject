{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaegnFARBl8UNumckalpmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caruanajoey/RedditMemeCoinPredictorProject/blob/main/ECON420FinalProjectRedditMemeCoinPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Project For ECON420 Machine Learning by Giuseppe Caruana 261115024"
      ],
      "metadata": {
        "id": "G5koOL8U3XXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "FcqL_3zrLm7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg29EDVfV-LT"
      },
      "outputs": [],
      "source": [
        "!pip install praw transformers torch tensorflow scikit-learn python-dotenv pycoingecko requests pandas numpy matplotlib seaborn langdetect ta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "6gT4Lba-LsYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from langdetect import detect\n",
        "\n",
        "# For Reddit\n",
        "import praw\n",
        "\n",
        "# For CoinGecko Market Data\n",
        "from pycoingecko import CoinGeckoAPI\n",
        "\n",
        "# For Sentiment Analysis\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ML + Deep Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Technical Indicators\n",
        "import ta\n",
        "\n",
        "# Disable unnecessary logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n"
      ],
      "metadata": {
        "id": "WXvTQ29mKqJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dictionary for CoinGeckoID"
      ],
      "metadata": {
        "id": "7zpJOwzeL61Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TICKER_MAP = {\n",
        "    \"DOGE\": \"dogecoin\",\n",
        "    \"SHIB\": \"shiba-inu\",\n",
        "    \"FLOKI\": \"floki\",\n",
        "    \"ELON\": \"dogelon-mars\",\n",
        "    \"PEPE\": \"pepe\",\n",
        "    \"BONK\": \"bonk\",\n",
        "    \"WIF\": \"dogwifcoin\",\n",
        "    \"PENGU\": \"pudgy-penguins\",\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "-kIxXy9JKuGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input for Coin Choice"
      ],
      "metadata": {
        "id": "b-d5YOipMQoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_ticker_choice():\n",
        "    #Prompt the user for a ticker symbol and validate it against TICKER_MAP.\n",
        "    print(\"Available Tickers:\")\n",
        "    for ticker in TICKER_MAP.keys():\n",
        "        print(f\"- {ticker}\")\n",
        "\n",
        "    coin_ticker = input(\"\\nEnter the meme coin ticker (e.g., DOGE, SHIB): \").strip().upper()\n",
        "    if coin_ticker not in TICKER_MAP:\n",
        "        print(f\"\\nTicker '{coin_ticker}' not found in TICKER_MAP. Please update or try another.\")\n",
        "        raise SystemExit\n",
        "    return coin_ticker\n",
        "\n"
      ],
      "metadata": {
        "id": "_DK14PpwKxdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit API info"
      ],
      "metadata": {
        "id": "ZtHQBhrlMgPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REDDIT_CLIENT_ID = \"RidrSLnK8tNcAtD8LNccNg\"\n",
        "REDDIT_CLIENT_SECRET = \"122v92C6r5WzMNNNQW3vg8KDIHsczA\"\n",
        "REDDIT_USER_AGENT = \"JoeysApp/0.0.1\"\n",
        "\n",
        "def get_reddit_client(client_id, client_secret, user_agent):\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "    return reddit\n"
      ],
      "metadata": {
        "id": "2J6v8wTTK_76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetching Data"
      ],
      "metadata": {
        "id": "4ureYv4wMlXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reddit data"
      ],
      "metadata": {
        "id": "ZgNpN2yjMqwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_comments_for_submission(submission, max_comments=20):\n",
        "    #Fetch up to 'max_comments'\n",
        "\n",
        "    submission.comments.replace_more(limit=0)\n",
        "    comment_texts = []\n",
        "    count = 0\n",
        "    for c in submission.comments:\n",
        "        if count >= max_comments:\n",
        "            break\n",
        "        comment_texts.append(c.body)\n",
        "        count += 1\n",
        "    return \" \".join(comment_texts)\n",
        "\n",
        "def fetch_reddit_data_enhanced(subreddit_name=\"memecoins\", total_posts=200,categories=(\"hot\",\"new\",\"top\"), max_comments=20, reddit_client=None):\n",
        "   #Returns a DataFrame with combined text (post + comments), upvotes, num_comments, category, timestamp.\n",
        "\n",
        "    if reddit_client is None:\n",
        "        raise ValueError(\"A valid PRAW Reddit client must be provided.\")\n",
        "\n",
        "    all_posts = []\n",
        "    subreddit = reddit_client.subreddit(subreddit_name)\n",
        "\n",
        "    for category in categories:\n",
        "        print(f\"\\n--- Fetching category='{category}' from r/{subreddit_name} with pagination ---\")\n",
        "        fetched_posts = 0\n",
        "        after_fullname = None\n",
        "\n",
        "        while fetched_posts < total_posts:\n",
        "            batch_limit = min(100, total_posts - fetched_posts)\n",
        "            params = {}\n",
        "            if after_fullname:\n",
        "                params[\"after\"] = after_fullname\n",
        "\n",
        "            listing = getattr(subreddit, category)(limit=batch_limit, params=params)\n",
        "\n",
        "            count_this_batch = 0\n",
        "            try:\n",
        "                for submission in listing:\n",
        "                    # Combine post text + up to 'max_comments' from the submission\n",
        "                    post_text = submission.selftext or \"\"\n",
        "                    comments_text = fetch_comments_for_submission(submission, max_comments=max_comments)\n",
        "                    combined_text = post_text + \"\\n\" + comments_text\n",
        "\n",
        "                    all_posts.append({\n",
        "                        \"timestamp\": datetime.utcfromtimestamp(submission.created_utc),\n",
        "                        \"title\": submission.title,\n",
        "                        \"text\": combined_text,\n",
        "                        \"upvotes\": submission.score,\n",
        "                        \"num_comments\": submission.num_comments,\n",
        "                        \"category\": category\n",
        "                    })\n",
        "\n",
        "                    after_fullname = submission.fullname  # pagination pointer\n",
        "                    fetched_posts += 1\n",
        "                    count_this_batch += 1\n",
        "\n",
        "                    if fetched_posts >= total_posts:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {category} posts: {e}\")\n",
        "                break\n",
        "\n",
        "            if count_this_batch == 0:\n",
        "                # No more posts returned, break\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(all_posts)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "rY72DBAfLhvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoinGecko data"
      ],
      "metadata": {
        "id": "jhaPNxujMta3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_coingecko_data(coin_id, vs_currency=\"usd\", days=7, interval=\"hourly\"):\n",
        "    cg = CoinGeckoAPI()\n",
        "    try:\n",
        "        # OHLC\n",
        "        ohlc = cg.get_coin_ohlc_by_id(id=coin_id, vs_currency=vs_currency, days=days)\n",
        "        df = pd.DataFrame(ohlc, columns=['timestamp','open','high','low','close'])\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "        # Volume\n",
        "        market_data = cg.get_coin_market_chart_by_id(id=coin_id, vs_currency=vs_currency, days=days)\n",
        "        volume = market_data.get('total_volumes', [])\n",
        "        vol_df = pd.DataFrame(volume, columns=['timestamp','volume'])\n",
        "        vol_df['timestamp'] = pd.to_datetime(vol_df['timestamp'], unit='ms')\n",
        "\n",
        "        # Merge\n",
        "        df = pd.merge_asof(df.sort_values('timestamp'),\n",
        "                           vol_df.sort_values('timestamp'),\n",
        "                           on='timestamp')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching from CoinGecko: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n"
      ],
      "metadata": {
        "id": "RO-sg6nFLgA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning up the data"
      ],
      "metadata": {
        "id": "6xj5PyGeM3i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s#@]', '', text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n"
      ],
      "metadata": {
        "id": "KtUC7s-MLemi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sentiment score"
      ],
      "metadata": {
        "id": "XAYKH6rqNATn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    if not text.strip():\n",
        "        return 0.0\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = sentiment_model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        return probs[1] - probs[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Sentiment error: {e}\")\n",
        "        return 0.0\n"
      ],
      "metadata": {
        "id": "LYpZ9EVyM-85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing and Merging Data"
      ],
      "metadata": {
        "id": "iMO1oEl6NXgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_merge_data(social_df, market_df):\n",
        "    social_df[\"timestamp\"] = pd.to_datetime(social_df[\"timestamp\"])\n",
        "    social_df.set_index(\"timestamp\", inplace=True)\n",
        "\n",
        "    social_hourly = social_df.resample(\"H\").agg({\n",
        "        \"sentiment_score\":\"mean\",\n",
        "        \"upvotes\":\"sum\",\n",
        "        \"num_comments\":\"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    social_hourly.rename(columns={\n",
        "        \"sentiment_score\":\"avg_sentiment_score\",\n",
        "        \"upvotes\":\"total_upvotes\",\n",
        "        \"num_comments\":\"total_comments\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    market_df[\"timestamp\"] = pd.to_datetime(market_df[\"timestamp\"])\n",
        "    market_df.sort_values(\"timestamp\", inplace=True)\n",
        "    market_df.set_index(\"timestamp\", inplace=True)\n",
        "\n",
        "    market_hourly = market_df.resample(\"H\").agg({\n",
        "        \"open\":\"first\",\n",
        "        \"high\":\"max\",\n",
        "        \"low\":\"min\",\n",
        "        \"close\":\"last\",\n",
        "        \"volume\":\"sum\"\n",
        "    }).dropna().reset_index()\n",
        "\n",
        "    merged_df = pd.merge_asof(\n",
        "        social_hourly.sort_values('timestamp'),\n",
        "        market_hourly.sort_values('timestamp'),\n",
        "        on='timestamp'\n",
        "    )\n",
        "    merged_df.dropna(inplace=True)\n",
        "    return merged_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NnPH79LqLb_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extra features"
      ],
      "metadata": {
        "id": "5KXjqgeXNqDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features(merged_df):\n",
        "\n",
        "    merged_df[\"lagged_sentiment\"] = merged_df[\"avg_sentiment_score\"].shift(1)\n",
        "    merged_df[\"close_sma_3\"] = merged_df[\"close\"].rolling(3).mean()\n",
        "\n",
        "    # RSI\n",
        "    merged_df[\"rsi\"] = ta.momentum.RSIIndicator(close=merged_df[\"close\"], window=14).rsi()\n",
        "\n",
        "    # Time-based features\n",
        "    merged_df[\"hour_of_day\"] = merged_df[\"timestamp\"].dt.hour\n",
        "    merged_df[\"day_of_week\"] = merged_df[\"timestamp\"].dt.dayofweek\n",
        "\n",
        "    merged_df.dropna(inplace=True)\n",
        "\n",
        "    # future_close\n",
        "    merged_df[\"future_close\"] = merged_df[\"close\"].shift(-1)\n",
        "    merged_df.dropna(inplace=True)\n",
        "\n",
        "    return merged_df\n"
      ],
      "metadata": {
        "id": "ooroYznzNoBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window Sequence"
      ],
      "metadata": {
        "id": "LfuczISINtjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_window_sequences(X, y, window_size=3):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - window_size):\n",
        "        X_seq.append(X[i:i+window_size])\n",
        "        y_seq.append(y[i+window_size])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n"
      ],
      "metadata": {
        "id": "a2EN4I_hLWEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24 hour forecast"
      ],
      "metadata": {
        "id": "ct5EUYvtOKn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_step_forecast_24hrs(model, scaler, last_data_scaled, window_size=3):\n",
        "    predictions_scaled = []\n",
        "\n",
        "    current_window = last_data_scaled.copy()\n",
        "\n",
        "    for step in range(24):\n",
        "        current_window_reshaped = np.expand_dims(current_window, axis=0)\n",
        "        pred_scaled = model.predict(current_window_reshaped).flatten()[0]\n",
        "        predictions_scaled.append(pred_scaled)\n",
        "        new_row = np.zeros((current_window.shape[1],))\n",
        "        new_row[-1] = pred_scaled\n",
        "        current_window = np.vstack([current_window[1:], new_row])\n",
        "\n",
        "    predictions_inversed = []\n",
        "    for ps in predictions_scaled:\n",
        "        placeholder = np.zeros((1, scaler.n_features_in_))\n",
        "        placeholder[0, -1] = ps\n",
        "        inv = scaler.inverse_transform(placeholder)\n",
        "        predictions_inversed.append(inv[0, -1])\n",
        "\n",
        "    return predictions_inversed\n",
        "\n"
      ],
      "metadata": {
        "id": "0Ut2SxhqLS6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "f0W9c6eTOPtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    coin_ticker = get_user_ticker_choice()\n",
        "    coingecko_id = TICKER_MAP[coin_ticker]\n",
        "\n",
        "    print(f\"\\nFetching data from r/memecoins with multiple categories, pagination, and comments ...\")\n",
        "    reddit_client = get_reddit_client(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT)\n",
        "\n",
        "    # total_posts=300 per category, categories = hot,new,top\n",
        "    reddit_df = fetch_reddit_data_enhanced(\n",
        "        subreddit_name=\"memecoins\",\n",
        "        total_posts=300,\n",
        "        categories=(\"hot\",\"new\",\"top\"),\n",
        "        max_comments=20,\n",
        "        reddit_client=reddit_client\n",
        "    )\n",
        "    if reddit_df.empty:\n",
        "        print(\"No data from r/memecoins. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFetching market data for {coin_ticker} from CoinGecko (id='{coingecko_id}') ...\")\n",
        "    market_df = fetch_coingecko_data(coin_id=coingecko_id, vs_currency=\"usd\", days=7, interval=\"hourly\")\n",
        "    if market_df.empty:\n",
        "        print(\"No market data from CoinGecko. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess\n",
        "    print(\"\\nCleaning & filtering r/memecoins data (English only)...\")\n",
        "    reddit_df.dropna(subset=[\"text\"], inplace=True)\n",
        "    reddit_df[\"cleaned_text\"] = reddit_df[\"text\"].apply(clean_text)\n",
        "    reddit_df = reddit_df[reddit_df[\"cleaned_text\"].apply(is_english)]\n",
        "\n",
        "    # Sentiment\n",
        "    print(\"Performing sentiment analysis (post + comments text) ...\")\n",
        "    reddit_df[\"sentiment_score\"] = reddit_df[\"cleaned_text\"].apply(get_sentiment_score)\n",
        "    if reddit_df.empty:\n",
        "        print(\"No posts left after cleaning/filtering. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Merge\n",
        "    print(\"Merging social data with market data ...\")\n",
        "    merged_df = process_and_merge_data(reddit_df, market_df)\n",
        "    if merged_df.shape[0] < 10:\n",
        "        print(\"Merged dataset too small for training. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 6) features\n",
        "    merged_df = features(merged_df)\n",
        "    print(f\"\\nFinal dataset shape: {merged_df.shape}\")\n",
        "    display(merged_df.head())\n",
        "\n",
        "    FEATURES = [\n",
        "        \"avg_sentiment_score\",\n",
        "        \"lagged_sentiment\",\n",
        "        \"close\",\n",
        "        \"close_sma_3\",\n",
        "        \"volume\",\n",
        "        \"rsi\",\n",
        "        \"hour_of_day\",\n",
        "        \"day_of_week\"\n",
        "    ]\n",
        "    TARGET = \"future_close\"\n",
        "\n",
        "    df_for_scale = merged_df[FEATURES + [TARGET]].copy()\n",
        "    data_np = df_for_scale.values\n",
        "\n",
        "    # Scale features + target\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data_np)\n",
        "\n",
        "    X_scaled = data_scaled[:, :-1]  # all but last col\n",
        "    y_scaled = data_scaled[:, -1]   # last col\n",
        "\n",
        "    # Train/Test Split\n",
        "    n = len(X_scaled)\n",
        "    split_index = int(n * 0.8)\n",
        "    X_train_scaled, X_test_scaled = X_scaled[:split_index], X_scaled[split_index:]\n",
        "    y_train_scaled, y_test_scaled = y_scaled[:split_index], y_scaled[split_index:]\n",
        "\n",
        "    # Create window sequences\n",
        "    WINDOW_SIZE = 3\n",
        "    X_train_seq, y_train_seq = create_window_sequences(X_train_scaled, y_train_scaled, window_size=WINDOW_SIZE)\n",
        "    X_test_seq, y_test_seq = create_window_sequences(X_test_scaled, y_test_scaled, window_size=WINDOW_SIZE)\n",
        "\n",
        "    print(f\"\\nTrain sequences shape: {X_train_seq.shape}, {y_train_seq.shape}\")\n",
        "    print(f\"Test sequences shape:  {X_test_seq.shape}, {y_test_seq.shape}\")\n",
        "\n",
        "    # Build LSTM\n",
        "    model = keras.Sequential([\n",
        "        layers.LSTM(64, return_sequences=True, input_shape=(WINDOW_SIZE, len(FEATURES))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(32),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.summary()\n",
        "\n",
        "    EPOCHS = 15\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    # Train\n",
        "    print(\"\\nTraining the LSTM model ...\")\n",
        "    history = model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluating on test set ...\")\n",
        "    y_pred_scaled = model.predict(X_test_seq).flatten()\n",
        "\n",
        "    # Inverse trans preds\n",
        "    def inverse_transform_predictions(scaled_array):\n",
        "        n_test = len(scaled_array)\n",
        "        placeholder = np.zeros((n_test, data_scaled.shape[1]))\n",
        "        placeholder[:, -1] = scaled_array\n",
        "        inv_data = scaler.inverse_transform(placeholder)\n",
        "        return inv_data[:, -1]\n",
        "\n",
        "    y_pred_inversed = inverse_transform_predictions(y_pred_scaled)\n",
        "    y_test_inversed = inverse_transform_predictions(y_test_seq)\n",
        "\n",
        "    # Eval\n",
        "    from math import sqrt\n",
        "    mse = mean_squared_error(y_test_inversed, y_pred_inversed)\n",
        "    rmse = sqrt(mse)\n",
        "    print(f\"\\nTest RMSE: {rmse:.5f}\")\n",
        "\n",
        "    # Visuals\n",
        "\n",
        "\n",
        "    # Plot Actual vs Predicted\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(range(len(y_test_inversed)), y_test_inversed, label='Actual Price', color='blue')\n",
        "    plt.plot(range(len(y_pred_inversed)), y_pred_inversed, label='Predicted Price', color='red')\n",
        "    plt.title(f\"{coin_ticker} Price Prediction (Test Set) - Inverse Transformed\")\n",
        "    plt.xlabel(\"Test Steps\")\n",
        "    plt.ylabel(\"Price (USD)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "    plt.title(\"Model Loss During Training\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss (MSE)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot error distribution\n",
        "    errors = y_test_inversed - y_pred_inversed\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.histplot(errors, kde=True, color='purple')\n",
        "    plt.title(\"Distribution of Prediction Errors (Test Set)\")\n",
        "    plt.xlabel(\"Error (Actual - Predicted)\")\n",
        "    plt.show()\n",
        "\n",
        "    last_data_window = X_scaled[-WINDOW_SIZE:, :]\n",
        "\n",
        "    next_24hrs = multi_step_forecast_24hrs(model, scaler, last_data_window, window_size=WINDOW_SIZE)\n",
        "\n",
        "    forecast_hours = list(range(1, 25))\n",
        "    forecast_df = pd.DataFrame({\n",
        "        \"Hour Ahead\": forecast_hours,\n",
        "        \"Predicted_Close\": next_24hrs\n",
        "    })\n",
        "\n",
        "    print(\"\\nPredicted prices for the next 24 hours (hourly):\")\n",
        "    display(forecast_df.head(24))\n",
        "\n",
        "    # The final predicted price after 24 hours\n",
        "    next_day_price = next_24hrs[-1]\n",
        "\n",
        "    print(f\"\\n\\033[95mThe predicted closing price for {coin_ticker} in ~24 hours is: ${next_day_price:.9f}\\033[0m\")\n",
        "    print(\"That's your next day's price estimate, based on the last known data.\\n\")\n"
      ],
      "metadata": {
        "id": "YeAUOOtMLC6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#running the main"
      ],
      "metadata": {
        "id": "KIguIYXhQFYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Y9K1EvJbKzd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Report\n",
        "\n"
      ],
      "metadata": {
        "id": "h-dPrwNrtJ6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was created to gather, process, and analyze data related to meme coins—cryptocurrencies known for their viral marketing and passionate online communities. The main goal is to determine whether combining social media sentiment with traditional market information can provide reliable short-term forecasts of a coin’s price. Essentially, the script follows three key steps: it collects social data from Reddit, retrieves corresponding price and volume data from CoinGecko, and merges everything into a dataset suitable for a deep learning model. By doing this, it aims to understand how collective opinions and discussions might influence real-time market behavior.\n",
        "\n",
        "Initially, I wanted to gather data from platforms like Twitter and TikTok. The idea was that a broader coverage would help identify trends that might not be visible when focusing on just one platform. Traders, both beginners and seasoned professionals, often use hashtags or short threads to share opinions that can sometimes move markets. Similarly, TikTok has surged in popularity among younger audiences, where videos can quickly go viral and capture the attention of young investors. Including these platforms seemed like a sure way to gather a richer source of sentiment data. However, I ran into some obstacles with the free API tiers offered by Twitter and TikTok, which made data gathering more complicated than expected. Twitter’s basic API access used to be fairly open, but recent policy changes and tighter restrictions have made it much harder to collect meaningful amounts of data without paying for higher-tier plans. With free access, you can only retrieve a limited number of tweets daily or monthly, severely limiting any potential analysis. TikTok, while extremely popular, doesn’t offer much in the way of official support for developers looking to gather large-scale data. I attempted to use unofficial scraping methods, but that came with its own challenges. Given these issues, it became clear that reliably fetching data from Twitter and TikTok at scale wasn’t practical with free plans or easily accessible APIs.\n",
        "\n",
        "With that realization, the focus was shifted to Reddit. Reddit turned out to be a much more manageable platform for gathering sentiment data. Its API is relatively open, and the platform naturally supports long-form discussions and comment threads. Instead of short tweets or videos, Reddit users typically post detailed submissions and replies. The script leverages Reddit’s Python library, PRAW, to collect posts from a chosen subreddit—in this case, “memecoins.” It also looks at multiple categories like “hot,” “new,” and “top” to cover a broader range of user-generated content. Once the data is collected, the script cleans up the text by removing links, special characters, and symbols that could interfere with language processing. It also filters out non-English content to keep the text data consistent and avoid translation issues. The final step in handling the text data involves sentiment analysis, where a pre-trained DistilBERT model is used to generate a continuous score that estimates how positive or negative each Reddit post is.\n",
        "\n",
        "On the market data side, the code relies on the CoinGecko API. Each meme coin has a unique ID on CoinGecko (like “dogecoin” or “shiba-inu”), and the script uses that ID to fetch the last several days of hourly price candles and trading volumes. This market data includes open, high, low, and close (OHLC) prices, which are standard metrics in trading analytics. After retrieving this information, the script combines it with the sentiment data. This merging is done based on the closest hourly timestamp, resulting in a single table that includes both social and market features. This gives a more complete picture of what might be influencing short-term price movements.\n",
        "\n",
        "After these steps, the script moves on to engineer additional features, such as moving averages of the coin’s closing price and the RSI (Relative Strength Index), a popular technical indicator. It also creates lagged versions of certain columns to ensure that past data can help predict future outcomes. By shifting the closing price forward, the script creates a “future_close” column, which becomes the target variable for the model to predict. The data is then normalized using the MinMaxScaler utility and is finally ready to be fed into a neural network. The chosen architecture is an LSTM, or Long Short-Term Memory model, which is well-suited for sequential data. The model includes two LSTM layers, each followed by dropout layers to help prevent overfitting. These layers handle the complexities of time dependencies in the data. The script then trains the model on the majority of the data, reserving a portion as a test set to evaluate the model’s performance on unseen data.\n",
        "\n",
        "There are several areas where the code could be improved. One obvious area is the breadth of sentiment analysis. Currently, the script uses a single pre-trained DistilBERT model to generate sentiment scores. While this approach is practical, it might sometimes misinterpret slang, sarcasm, or crypto-specific jargon. Training a specialized model or at least fine-tuning the existing model for the crypto domain could provide more accurate and detailed sentiment scores, especially since meme coin communities often rely heavily on humor and memes. Moreover, the script could benefit from integrating additional sources of data beyond just Reddit and CoinGecko. For example, incorporating news articles, blog posts, or other forms of online content could provide a more comprehensive view of the factors influencing meme coin prices. Combining these various data streams could help the model capture a wider array of signals that drive market behavior.\n",
        "\n",
        "In summary, this code began with a broader vision of incorporating data from Twitter, TikTok, and Reddit to capture the full diversity of online chatter around meme coins. However, practical limitations imposed by the free APIs of Twitter and TikTok forced a pivot to focus solely on Reddit. Despite this setback, Reddit proved to be a valuable and accessible source for text-based sentiment data. By pairing this social sentiment data with CoinGecko’s market information, the code successfully creates a dynamic dataset that a deep learning model can use to predict short-term price changes. While there are clear opportunities for enhancements—such as refining sentiment analysis, expanding the range of technical features, or fine-tuning hyperparameters—the existing script provides a functional and insightful example of how social media sentiment can be integrated into a traditional market prediction pipeline.\n"
      ],
      "metadata": {
        "id": "jElsXb3qzXgw"
      }
    }
  ]
}